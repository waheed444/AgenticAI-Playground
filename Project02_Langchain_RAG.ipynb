{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyORT44HTlKc/oFwnCzF0pbk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/waheed444/AgenticAI-Playground/blob/main/Project02_Langchain_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set-up the Environment & Install Prerequisites"
      ],
      "metadata": {
        "id": "u0hYYdOBbOfI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -q --upgrade google-generativeai langchain langchain-community langchain-google-genai chromadb pypdf\n"
      ],
      "metadata": {
        "id": "FJjhol8OWKtV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d55afbf4-bde7-46d6-c1b2-9bdcac5100b8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m605.5/605.5 kB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m67.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.6/278.6 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m64.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.6/411.6 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.2/93.2 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m81.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.9/54.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.1/442.1 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m443.8/443.8 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.17.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.29.2 which is incompatible.\n",
            "tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 5.29.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Display the output in  Prope Formate"
      ],
      "metadata": {
        "id": "KvfemeSDzFmh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display\n",
        "from IPython.display import Markdown\n",
        "import textwrap\n",
        "\n",
        "\n",
        "def to_markdown(text):\n",
        "  text = text.replace('•', '  *')\n",
        "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))"
      ],
      "metadata": {
        "id": "xneaxYvGx-TI"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import **genai** & Configure **API Key**"
      ],
      "metadata": {
        "id": "hVFySiHVb1mk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ],
      "metadata": {
        "id": "iNexSDVoX4Bf"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import **Libraries** to build QnA using **LangChain and PDF Data** Integration"
      ],
      "metadata": {
        "id": "4VF2zd-hcSGk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib\n",
        "import warnings\n",
        "from pathlib import Path as p\n",
        "from pprint import pprint\n",
        "\n",
        "import pandas as pd\n",
        "from langchain import PromptTemplate\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "# restart python kernal if issues with langchain import."
      ],
      "metadata": {
        "id": "61slydEBYQvp"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI"
      ],
      "metadata": {
        "id": "ovwlpefMYjZu"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configure **gimini-pro** model with custom parameters"
      ],
      "metadata": {
        "id": "hKI81AUscmTR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = ChatGoogleGenerativeAI(model=\"gemini-pro\",google_api_key=GOOGLE_API_KEY,temperature=0.6,convert_system_message_to_human=True)"
      ],
      "metadata": {
        "id": "t3Cc55WvYnu2"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checking file Existence using **pathlib** library"
      ],
      "metadata": {
        "id": "fh9JZw9_c8Ca"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "file_path = Path(\"/content/RAG_File.pdf\")\n",
        "if file_path.exists():\n",
        "    print(\"File exists and is ready for processing.\")\n",
        "else:\n",
        "    print(\"File does not exist. Please upload it.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0LDRScHMYuKX",
        "outputId": "1efb0568-04d4-4cf5-8aaa-efb7b76c42ad"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File exists and is ready for processing.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading & Splitting PDF Content Using **PyPDFLoader** library"
      ],
      "metadata": {
        "id": "7vMd4-UGdKZW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "pdf_loader = PyPDFLoader(\"/content/RAG_File.pdf\")\n",
        "pages = pdf_loader.load_and_split()\n",
        "print(pages[3].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LDCyrPEfZZVa",
        "outputId": "7767c3c1-e3d1-4e54-922c-7dec6e37113d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "technologies.Useopensourcelibraries,likeLangchain,CrewAI,andLangGraphtoautomaterepeatable,multi-steptasksandautomatebusinessprocessesthataretypicallydonebyagroupofpeople.\n",
            "Certifications:\n",
            "■ MicrosoftCertified:AzureAIEngineerAssociate■ CertifiedcrewAIEngineer\n",
            "LearningRepo:https://github.com/panaversity/learn-prompt-eng-gpts-ai-agents\n",
            "● Quarter3:CloudNativeAIPoweredMicroservicesDesign,Development,andDeployment:\n",
            "BuildscalableAIPoweredAPIsusingFastAPI,Postgres,Kafka,Kong,GenAIAPIslikeOpenAIChatCompletionAPIs,AssistantAPIs,LangChainandOpenSourceAILLMs,developthemusingContainersandDevContainers,anddeploythemusingDockerComposelocallyandKubernetesPoweredServerlessContainerServicesonthecloud.\n",
            "WewillalsolearntointegratedesignthinkingandBehavior-DrivenDevelopment(BDD)indevelopingAIsystems.WewilllearntocreateAIsolutionsthataredeeplyalignedwithuserneedsandexpectations.Designthinkingensuresathoroughunderstandingoftheuserandproblemspace,whileBDDprovidesastructuredapproachtodefiningandvalidatingthedesiredbehavioursoftheAIsystem.Together,thesemethodologiesleadtothedevelopmentofAIsolutionsthatarenotonlytechnicallyrobustbutalsohighlyuser-centricandeffectiveinsolvingreal-worldproblems.\n",
            "○ Certifications:■ PostgreSQL13AssociateCertification■ ConfluentCertifiedDeveloperforApacheKafka(CCDAK)■ DesignThinkingProfessionalCertificate(DTPC)■ TestandBehaviorDrivenDevelopment(TDD/BDD)\n",
            "LearningRepo:\n",
            "https://github.com/panaversity/learn-cloud-native-ai-powered-microservices/\n",
            "4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Splitting Text into Chunks for Embedding Using **LangChain and Google Generative AI**"
      ],
      "metadata": {
        "id": "DBp25UiJdWM1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings"
      ],
      "metadata": {
        "id": "JTiFXg_iZ5c9"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=1000)\n",
        "context = \"\\n\\n\".join(str(p.page_content) for p in pages)\n",
        "texts = text_splitter.split_text(context)"
      ],
      "metadata": {
        "id": "knsk5_P1Z7pq"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\",google_api_key=GOOGLE_API_KEY)"
      ],
      "metadata": {
        "id": "BmVcnw1GZ-LP"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating a Retrieval-Based QnA System with **Google Generative AI Embeddings and Chroma**"
      ],
      "metadata": {
        "id": "_jOL7AO7d-5P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vector_index = Chroma.from_texts(texts, embeddings).as_retriever(search_kwargs={\"k\":5})\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    model,\n",
        "    retriever=vector_index,\n",
        "    return_source_documents=True\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "waGtbc8kaAmf"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Querying and Displaying Output"
      ],
      "metadata": {
        "id": "eVTgVIXVePZ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What are the main topics discussed in this file?\"\n",
        "result = qa_chain({\"query\": question})\n",
        "result[\"result\"]\n",
        "Markdown(result[\"result\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "AKy2Gf1kaHZY",
        "outputId": "c575495f-14e9-4fc4-d99b-ce10ce6761ec"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "This file discusses the following topics:\n1. Autonomy of AIAgents\n2. Learning and Adaptation of AIAgents\n3. Interactivity of AIAgents\n4. Design Thinking and Behavior-Driven Development for AIAgents and Custom GPTs\n5. Fine-Tuning Open-Source Large Language Models with FastAI and PyTorch\n6. Importance of PyTorch and FastAI in Fine-Tuning LLMs\n7. Why TensorFlow is not preferred over PyTorch and FastAI for fine-tuning LLMs\n8. Challenges in Developing LLMs like ChatGPT4 or Google Gemini\n9. Business Considerations for Developing Custom LLMs vs Using Existing LLMs\n10. What are Custom GPTs?\n11. What are Actions in GPTs?\n12. Differences between AIAgents and Custom GPTs\n13. Data Preparation for Fine-Tuning MetaLLaMA3 with PyTorch\n14. Fine-Tuning MetaLLaMA3 with PyTorch\n15. Utilizing FastAI for NLP Tasks\n16. Cloud-Native Training and Deployment for Fine-Tuned LLMs\n17. Exporting Models for Inference and Building Inference Pipelines\n18. Capstone Project for Fine-Tuning and Deploying MetaLLaMA3\n19. Physical AI and Humanoid Robotics Development\n20. Cloud Native Microservices Deployment and Distributed System Design\n21. Front-end Web GUI Development using Next.js and TypeScript\n22. What is Cloud Native Applied Generative AI Engineering?\n23. What is Physical AI?\n24. Different Specializations offered at the end of the program and their benefits\n25. Purpose of Docker Containers and benefits of deploying them with Docker Compose and Kubernetes\n26. Purpose of learning to develop APIs in a Generative AI program\n27. Purpose of using Python-based FastAPI and related technologies in Quarter 3\n28. API-as-a-Product model\n29. Benefits of using Docker Containers for development, testing, and deployment\n30. Advantages of using open Docker, Kubernetes, and Terraform technologies instead of using AWS, Azure, or Google Cloud technologies\n31. Reasons for not learning to build LLMs in the program and the difficulty of developing LLMs like ChatGPT4 or Google Gemini"
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    }
  ]
}